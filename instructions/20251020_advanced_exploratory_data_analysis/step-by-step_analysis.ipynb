{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3fe5c6",
   "metadata": {},
   "source": [
    "# Step-by-Step PySpark Analysis\n",
    "\n",
    "This notebook provides a guided example that loads the Parquet files from `batch_001` and `batch_002`, combining them into a single PySpark `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f265e9",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "- Run `pip install -r requirements.txt` to ensure `pyspark` and `pyarrow` are available.\n",
    "- Start `docker compose` to bring MinIO online in case you want to push data to object storage later.\n",
    "- This notebook assumes the files live in the lesson directory `data_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4e5240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName('Advanced-EDA-Step-By-Step')\n",
    "    .config('spark.sql.session.timeZone', 'UTC')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6513076d",
   "metadata": {},
   "source": [
    "## 2. Defining working paths\n",
    "Organize references for each batch directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b46ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT = Path('instructions') / '20251020_advanced_exploratory_data_analysis' / 'data_test'\n",
    "BATCH_001 = DATA_ROOT / 'batch_001'\n",
    "BATCH_002 = DATA_ROOT / 'batch_002'\n",
    "\n",
    "print('Batch 1 ->', BATCH_001.resolve())\n",
    "print('Batch 2 ->', BATCH_002.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482044e6",
   "metadata": {},
   "source": [
    "## 3. Inspecting available tables\n",
    "List the subfolders (each represents one Parquet table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e77e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = sorted([p.name for p in BATCH_001.iterdir() if p.is_dir()])\n",
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e1fe1",
   "metadata": {},
   "source": [
    "## 4. Loading a specific table\n",
    "Use `efentradas` as the running example: load each batch and then union the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b014462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'efentradas'\n",
    "\n",
    "batch1_df = spark.read.parquet(str(BATCH_001 / table_name))\n",
    "batch2_df = spark.read.parquet(str(BATCH_002 / table_name))\n",
    "\n",
    "combined_df = batch1_df.unionByName(batch2_df, allowMissingColumns=True)\n",
    "\n",
    "combined_df.printSchema()\n",
    "combined_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2ea80",
   "metadata": {},
   "source": [
    "## 5. Utility function to combine tables\n",
    "Build a helper that returns a merged `DataFrame` for any table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30010809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def load_table(table: str) -> DataFrame:\n",
    "    paths = [BATCH_001 / table, BATCH_002 / table]\n",
    "    dataframes = []\n",
    "    for path in paths:\n",
    "        if not path.exists():\n",
    "            print(f'Aviso: {path} n√£o encontrado; ignorando este lote.')\n",
    "            continue\n",
    "        dataframes.append(spark.read.parquet(str(path)))\n",
    "\n",
    "    if not dataframes:\n",
    "        raise FileNotFoundError(f'Nenhum dado encontrado para a tabela {table}.')\n",
    "\n",
    "    merged = dataframes[0]\n",
    "    for df in dataframes[1:]:\n",
    "        merged = merged.unionByName(df, allowMissingColumns=True)\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b129966c",
   "metadata": {},
   "source": [
    "## 6. Applying the helper across tables\n",
    "Load a few tables and review counts to make sure the union worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e336b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []\n",
    "for table in ['efentradas', 'efsaidas', 'ctlancto']:\n",
    "    df = load_table(table)\n",
    "    summary.append((table, df.count()))\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78884d69",
   "metadata": {},
   "source": [
    "## 7. Exploratory profiling for `ctcontas`\n",
    "Focus on the `ctcontas` (chart of accounts) table to validate that both batches stitch together correctly and to capture quick business signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98870f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "table = 'ctcontas'\n",
    "\n",
    "batch1_ct = spark.read.parquet(str(BATCH_001 / table))\n",
    "batch2_ct = spark.read.parquet(str(BATCH_002 / table))\n",
    "\n",
    "print(f\"Batch 001 rows: {batch1_ct.count()}\")\n",
    "print(f\"Batch 002 rows: {batch2_ct.count()}\")\n",
    "\n",
    "ctcontas_df = load_table(table).cache()\n",
    "print(f\"Combined rows: {ctcontas_df.count()}\")\n",
    "\n",
    "ctcontas_df.agg(\n",
    "    F.countDistinct('conta_contabil').alias('distinct_accounts'),\n",
    "    F.countDistinct('descricao').alias('distinct_descriptions'),\n",
    "    F.countDistinct('natureza').alias('distinct_natures'),\n",
    "    F.min('nivel').alias('min_level'),\n",
    "    F.max('nivel').alias('max_level')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc4535",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ctcontas_df\n",
    "    .groupBy('natureza')\n",
    "    .agg(\n",
    "        F.count('*').alias('rows'),\n",
    "        F.countDistinct('conta_contabil').alias('distinct_accounts')\n",
    "    )\n",
    "    .orderBy('natureza')\n",
    "    .show()\n",
    ")\n",
    "\n",
    "(\n",
    "    ctcontas_df\n",
    "    .groupBy('nivel')\n",
    "    .count()\n",
    "    .orderBy('nivel')\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e96ac0",
   "metadata": {},
   "source": [
    "## 8. Visualizing distributions\n",
    "This dataset is small enough to collect locally. Convert to Pandas and sketch quick plots. If `matplotlib` is not installed, run `pip install matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da7491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ctcontas_pd = ctcontas_df.toPandas()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ctcontas_pd['nivel'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='#4c78a8')\n",
    "axes[0].set_title('Account level distribution')\n",
    "axes[0].set_xlabel('Level')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "ctcontas_pd['natureza'].value_counts().sort_values(ascending=True).plot(kind='barh', ax=axes[1], color='#f58518')\n",
    "axes[1].set_title('Account nature distribution')\n",
    "axes[1].set_xlabel('Count')\n",
    "axes[1].set_ylabel('Nature')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64373952",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctcontas_df.orderBy('conta_contabil').show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b2b7bf",
   "metadata": {},
   "source": [
    "## 9. Saving or pushing to MinIO\n",
    "After validating, persist the `DataFrames` back to Parquet, either locally or in MinIO via the `s3a://` protocol.\n",
    "\n",
    "```python\n",
    "(df.write\n",
    "    .mode('overwrite')\n",
    "    .format('parquet')\n",
    "    .save('s3a://<bucket>/refined/efentradas'))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01446772",
   "metadata": {},
   "source": [
    "## 10. Stopping the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8fd580",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
