{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3fe5c6",
   "metadata": {},
   "source": [
    "# Step-by-Step PySpark Analysis\n",
    "\n",
    "This notebook provides a guided example that loads the Parquet files from `batch_001` and `batch_002`, combining them into a single PySpark `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f265e9",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "- Run `pip install -r requirements.txt` to ensure `pyspark` and `pyarrow` are available.\n",
    "- Start `docker compose` to bring MinIO online in case you want to push data to object storage later.\n",
    "- This notebook assumes the files live in the lesson directory `data_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a4e5240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Advanced-EDA-Step-By-Step</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21f7eb11ee0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName('Advanced-EDA-Step-By-Step')\n",
    "    .config('spark.sql.session.timeZone', 'UTC')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6513076d",
   "metadata": {},
   "source": [
    "## 2. Defining working paths\n",
    "Organize references for each batch directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b46ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 -> C:\\Users\\Inteli\\Documents\\GitHub\\20254si08\\instructions\\20251020_advanced_exploratory_data_analysis\\data_test\\batch_001\n",
      "Batch 2 -> C:\\Users\\Inteli\\Documents\\GitHub\\20254si08\\instructions\\20251020_advanced_exploratory_data_analysis\\data_test\\batch_002\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT = Path('instructions') / \n",
    "BATCH_001 = DATA_ROOT / 'batch_001'\n",
    "BATCH_002 = DATA_ROOT / 'batch_002'\n",
    "\n",
    "print('Batch 1 ->', BATCH_001.resolve())\n",
    "print('Batch 2 ->', BATCH_002.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482044e6",
   "metadata": {},
   "source": [
    "## 3. Inspecting available tables\n",
    "List the subfolders (each represents one Parquet table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448e77e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] O sistema não pode encontrar o caminho especificado: 'instructions\\\\20251020_advanced_exploratory_data_analysis\\\\data_test\\\\batch_001'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tables = \u001b[38;5;28msorted\u001b[39m(\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mBATCH_001\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m      2\u001b[39m tables\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\pathlib.py:1056\u001b[39m, in \u001b[36mPath.iterdir\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1050\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34miterdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1051\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Yield path objects of the directory contents.\u001b[39;00m\n\u001b[32m   1052\u001b[39m \n\u001b[32m   1053\u001b[39m \u001b[33;03m    The children are yielded in arbitrary order, and the\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[33;03m    special entries '.' and '..' are not included.\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m   1057\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_child_relpath(name)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] O sistema não pode encontrar o caminho especificado: 'instructions\\\\20251020_advanced_exploratory_data_analysis\\\\data_test\\\\batch_001'"
     ]
    }
   ],
   "source": [
    "tables = sorted([p.name for p in BATCH_001.iterdir() if p.is_dir()])\n",
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e1fe1",
   "metadata": {},
   "source": [
    "## 4. Loading a specific table\n",
    "Use `efentradas` as the running example: load each batch and then union the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b014462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'efentradas'\n",
    "\n",
    "batch1_df = spark.read.parquet(str(BATCH_001 / table_name))\n",
    "batch2_df = spark.read.parquet(str(BATCH_002 / table_name))\n",
    "\n",
    "combined_df = batch1_df.unionByName(batch2_df, allowMissingColumns=True)\n",
    "\n",
    "combined_df.printSchema()\n",
    "combined_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2ea80",
   "metadata": {},
   "source": [
    "## 5. Utility function to combine tables\n",
    "Build a helper that returns a merged `DataFrame` for any table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30010809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def load_table(table: str) -> DataFrame:\n",
    "    paths = [BATCH_001 / table, BATCH_002 / table]\n",
    "    dataframes = []\n",
    "    for path in paths:\n",
    "        if not path.exists():\n",
    "            print(f'Aviso: {path} não encontrado; ignorando este lote.')\n",
    "            continue\n",
    "        dataframes.append(spark.read.parquet(str(path)))\n",
    "\n",
    "    if not dataframes:\n",
    "        raise FileNotFoundError(f'Nenhum dado encontrado para a tabela {table}.')\n",
    "\n",
    "    merged = dataframes[0]\n",
    "    for df in dataframes[1:]:\n",
    "        merged = merged.unionByName(df, allowMissingColumns=True)\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b129966c",
   "metadata": {},
   "source": [
    "## 6. Applying the helper across tables\n",
    "Load a few tables and review counts to make sure the union worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e336b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []\n",
    "for table in ['efentradas', 'efsaidas', 'ctlancto']:\n",
    "    df = load_table(table)\n",
    "    summary.append((table, df.count()))\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78884d69",
   "metadata": {},
   "source": [
    "## 7. Exploratory profiling for `ctcontas`\n",
    "Focus on the `ctcontas` (chart of accounts) table to validate that both batches stitch together correctly and to capture quick business signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98870f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "table = 'ctcontas'\n",
    "\n",
    "batch1_ct = spark.read.parquet(str(BATCH_001 / table))\n",
    "batch2_ct = spark.read.parquet(str(BATCH_002 / table))\n",
    "\n",
    "print(f\"Batch 001 rows: {batch1_ct.count()}\")\n",
    "print(f\"Batch 002 rows: {batch2_ct.count()}\")\n",
    "\n",
    "ctcontas_df = load_table(table).cache()\n",
    "print(f\"Combined rows: {ctcontas_df.count()}\")\n",
    "\n",
    "ctcontas_df.agg(\n",
    "    F.countDistinct('conta_contabil').alias('distinct_accounts'),\n",
    "    F.countDistinct('descricao').alias('distinct_descriptions'),\n",
    "    F.countDistinct('natureza').alias('distinct_natures'),\n",
    "    F.min('nivel').alias('min_level'),\n",
    "    F.max('nivel').alias('max_level')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc4535",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ctcontas_df\n",
    "    .groupBy('natureza')\n",
    "    .agg(\n",
    "        F.count('*').alias('rows'),\n",
    "        F.countDistinct('conta_contabil').alias('distinct_accounts')\n",
    "    )\n",
    "    .orderBy('natureza')\n",
    "    .show()\n",
    ")\n",
    "\n",
    "(\n",
    "    ctcontas_df\n",
    "    .groupBy('nivel')\n",
    "    .count()\n",
    "    .orderBy('nivel')\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e96ac0",
   "metadata": {},
   "source": [
    "## 8. Visualizing distributions\n",
    "This dataset is small enough to collect locally. Convert to Pandas and sketch quick plots. If `matplotlib` is not installed, run `pip install matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da7491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ctcontas_pd = ctcontas_df.toPandas()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ctcontas_pd['nivel'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='#4c78a8')\n",
    "axes[0].set_title('Account level distribution')\n",
    "axes[0].set_xlabel('Level')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "ctcontas_pd['natureza'].value_counts().sort_values(ascending=True).plot(kind='barh', ax=axes[1], color='#f58518')\n",
    "axes[1].set_title('Account nature distribution')\n",
    "axes[1].set_xlabel('Count')\n",
    "axes[1].set_ylabel('Nature')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64373952",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctcontas_df.orderBy('conta_contabil').show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b2b7bf",
   "metadata": {},
   "source": [
    "## 9. Saving or pushing to MinIO\n",
    "After validating, persist the `DataFrames` back to Parquet, either locally or in MinIO via the `s3a://` protocol.\n",
    "\n",
    "```python\n",
    "(df.write\n",
    "    .mode('overwrite')\n",
    "    .format('parquet')\n",
    "    .save('s3a://<bucket>/refined/efentradas'))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01446772",
   "metadata": {},
   "source": [
    "## 10. Stopping the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8fd580",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
